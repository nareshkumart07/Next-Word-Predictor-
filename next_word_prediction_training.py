# -*- coding: utf-8 -*-
"""Next_word_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d821th418vfRRoXkiuRmWtewEwTrKdOd

--**Next Word Prediction(GRU) Model ** --
"""

# importing required libiries

import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.optim import Optimizer
from collections import Counter
import numpy as np
import re

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# importing file
with open("/content/Pride and Prejudice by Jane Asus.txt", "r", encoding="utf-8") as f:
    text = f.read()

# Step - 01 preprocessing

# converting into lowercase
text = text.lower()
# removing punctuation
text = re.sub(r"[^a-zA-Z0-9\s]", "", text)
# remoing nubers
text = re.sub(r"\d+", "", text)
# removing symbols
text = re.sub(r"[^\w\s]", "", text)
# tokenization
words = text.split()

# removing stop words

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
words = [word for word in words if word not in stop_words]

# Step 02 - Creating vocubalary

word_count = Counter(words)  # this count words frequency
sorted_vocab = sorted(word_count, key=word_count.get, reverse=True)
word2idx = {word: idx for idx, word in enumerate(sorted_vocab)}
idx2word = {idx: word for word, idx in word2idx.items()}

vocab = word2idx
vocab["<pad>"] = 0
vocab["<unk>"] = 1

# Step 03 - Defining Model Architecture

class Model(nn.Module):
  def __init__(self, input_size,embedding_dim, hidden_dim, num_layers):
    super(Model, self).__init__()
    self.embedding = nn.Embedding(input_size, embedding_dim)
    self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True,bidirectional= False)
    self.fc = nn.Linear(hidden_dim, input_size)

  def forward(self,x):
    x = self.embedding(x)
    output,hidden = self.gru(x)
    output = self.fc(output[:,-1,:])
    return output

# Step 04 - Create Input-Target Pairs for Next-Word Prediction

sequence_length = 5  # 4 input + 1 target
sequences = []

for i in range(sequence_length, len(words)):
    seq = words[(i - sequence_length):i]
    # Convert words to indices using word2idx
    seq_indices = [word2idx.get(word, word2idx["<unk>"]) for word in seq]
    sequences.append(seq_indices)

# Now, sequences contains lists of word indices, which can be converted to a tensor
sequences = torch.tensor(sequences)  # shape: (num_samples, sequence_length)
X = sequences[:, :-1]  # first 4 words
y = sequences[:, -1]   # 5th word (target)

# Step 05 - Initilizing the model

vocab_size = len(vocab)
embedding_dim = 128
hidden_dim = 256
num_layers = 2

model = Model(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)
print(model)

# Step 06 - Defining loss and optimizer

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Step 07 - Training the model

# Training Loop
num_epochs = 10
batch_size = 64

for epoch in range(num_epochs):
  model.train()
  for i in range(0, len(X), batch_size):
    X_batch = X[i:i+batch_size].to(device)
    y_batch = y[i:i+batch_size].to(device)

    optimizer.zero_grad()
    outputs = model(X_batch)
    loss = criterion(outputs, y_batch)
    loss.backward()
    optimizer.step()

  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

print("Training finished.")

# Step 08 - Calculate perplexity of the model

import torch
import torch.nn as nn
from torch.nn import functional as F

# Assuming your model and data are already defined as in your previous code

def calculate_perplexity(model, X, y, batch_size=64):
    """Calculates the perplexity of the model."""
    model.eval()  # Set the model to evaluation mode
    total_loss = 0
    with torch.no_grad():
        for i in range(0, len(X), batch_size):
            X_batch = X[i:i+batch_size].to(device)
            y_batch = y[i:i+batch_size].to(device)

            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            total_loss += loss.item() * len(X_batch) # Scale loss by batch size

    avg_loss = total_loss / len(X)
    perplexity = torch.exp(torch.tensor(avg_loss))
    return perplexity.item()

# Example usage
perplexity = calculate_perplexity(model, X, y)
print(f"Perplexity: {perplexity}")

"""**Saving The Trained Model**"""

# Storing the trined model

import pickle

filename = "next_word_prediction.sav"
pickle.dump(model, open(filename, "wb"))

# Storing the required files
fname2 = "idx2word.txt"
with open(fname2, "w") as f:
    for key, value in idx2word.items():
        f.write(f"{key}: {value}\n")

fname1 = "word2idx.txt"
with open(fname1, "w") as f:
    for key, value in word2idx.items():
        f.write(f"{key}: {value}\n")

"""**Using Trained Model and Predicting Next Model**"""

import pickle

load_model = pickle.load(open("/content/next_word_prediction.sav", "rb"))

# loading files

with open('/content/idx2word.txt', 'r') as f1:
  idx2word_content = f1.read()


with open('/content/word2idx.txt', 'r') as f2:
  word2idx_content = f2.read()

# coverting above text file into dict

idx2word = {}
for line in idx2word_content.splitlines():
  key, value = line.split(': ')
  idx2word[int(key)] = value.strip()

word2idx = {}
for line in word2idx_content.splitlines():
  key, value = line.split(': ')
  word2idx[key.strip()] = int(value)

# defining function for prepocessing

import torch
import torch.nn.functional as F

def preprocess_input(text, word2idx, max_seq_len):
    """
    Tokenizes and pads input text.

    Args:
    - text (str): raw input sequence
    - word2idx (dict): mapping from word to index
    - max_seq_len (int): max input length

    Returns:
    - torch.Tensor: tokenized and padded sequence
    """
    text = text.lower().strip().split()
    tokenized = [word2idx.get(word, word2idx["<unk>"]) for word in text]

    # Pad with zeros from the left
    if len(tokenized) < max_seq_len:
        tokenized = [word2idx["<pad>"]] * (max_seq_len - len(tokenized)) + tokenized
    else:
        tokenized = tokenized[-max_seq_len:]

    return torch.tensor(tokenized).unsqueeze(0)  # shape: (1, max_seq_len)

# Defing the function which can take predict next word

def predict_next_word_gru(input_text, model, word2idx, idx2word, max_seq_len = 4, device="cpu"):
    """
    Predicts the next word using a trained GRU model.

    Args:
    - input_text (str): user-provided input sequence
    - model (torch.nn.Module): trained GRU model
    - word2idx (dict): word-to-index mapping
    - idx2word (dict): index-to-word mapping
    - max_seq_len (int): max input sequence length
    - device (str): 'cpu' or 'cuda'

    Returns:
    - str: predicted next word
    """

    model.eval()
    model.to(device)

    # 1. Preprocess and tokenize
    input_seq = preprocess_input(input_text, word2idx, max_seq_len).to(device)

    # 2. Run the model
    with torch.no_grad():
        output = load_model(input_seq)  # output shape: (batch_size, vocab_size)

    # 3. Pick the word with the highest score
    predicted_idx = torch.argmax(output, dim=-1).item()

    # 4. Convert index to word
    predicted_word = idx2word.get(predicted_idx, "<unk>")

    return predicted_word

# Taking input from user

input_text = input("Enter a sequence of words: ")
predicted = predict_next_word_gru(input_text, load_model, word2idx, idx2word, max_seq_len)
print("Predicted word:", predicted)